{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import gym\n",
    "import random\n",
    "from config import ENV_NAME\n",
    "from process_image import process_image\n",
    "from plugin_write_and_run import write_and_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/replay_buffer.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/replay_buffer.py\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Replay Memory that stores the last size transitions\n",
    "    \"\"\"\n",
    "    def __init__(self, size: int=1000000, input_shape: tuple=(84, 84), history_length: int=4, reward_type: str = \"integer\"):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            size: Number of stored transitions\n",
    "            input_shape: Shape of the preprocessed frame\n",
    "            history_length: Number of frames stacked together that the agent can see\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "        self.count = 0  # total index of memory written to, always less than self.size\n",
    "        self.current = 0  # index to write to\n",
    "\n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.input_shape[0], self.input_shape[1]), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        self.priorities = np.zeros(self.size, dtype=np.float32)\n",
    "\n",
    "        self.reward_type = reward_type\n",
    "\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True, reward_type=\"integer\"):\n",
    "        \"\"\"Saves a transition to the replay buffer\n",
    "\n",
    "        Arguments:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of the game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != self.input_shape:\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "\n",
    "        if clip_reward:\n",
    "            if reward_type == \"integer\":\n",
    "                reward = np.sign(reward)\n",
    "            else:\n",
    "                reward = np.clip(reward, -1.0, 1.0)\n",
    "        # Write memory\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.priorities[self.current] = max(self.priorities.max(), 1)  # make the most recent experience important\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size # when a < b then a % b = a\n",
    "\n",
    "    def get_minibatch(self, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Returns a minibatch of size batch_size\n",
    "\n",
    "        Arguments:\n",
    "            batch_size: How many samples to return\n",
    "\n",
    "        Returns:\n",
    "            A tuple of states, actions, rewards, new_states, and terminals\n",
    "        \"\"\"\n",
    "\n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "\n",
    "        indices = []\n",
    "        for i in range(batch_size):\n",
    "            while True:\n",
    "                # Get a random number from history_length to maximum frame\n",
    "                index = random.randint(self.history_length, self.count - 1)\n",
    "\n",
    "                # We check that all frames are from same episode with the two following if statements.  If either are True, the index is invalid.\n",
    "                if index >= self.current and index - self.history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            indices.append(index)\n",
    "\n",
    "        # Retrieve states from memory\n",
    "        states = []\n",
    "        new_states = []\n",
    "        for idx in indices:\n",
    "            states.append(self.frames[idx-self.history_length:idx, ...])\n",
    "            new_states.append(self.frames[idx-self.history_length+1:idx+1, ...])\n",
    "\n",
    "        states = np.transpose(np.asarray(states), axes=(0, 2, 3, 1))\n",
    "        new_states = np.transpose(np.asarray(new_states), axes=(0, 2, 3, 1))\n",
    "\n",
    "        return states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]\n",
    "\n",
    "    def save(self, folder_name):\n",
    "        \"\"\"\n",
    "        Save the replay buffer\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "\n",
    "        np.save(folder_name + '/actions.npy', self.actions)\n",
    "        np.save(folder_name + '/frames.npy', self.frames)\n",
    "        np.save(folder_name + '/rewards.npy', self.rewards)\n",
    "        np.save(folder_name + '/terminal_flags.npy', self.terminal_flags)\n",
    "\n",
    "    def load(self, folder_name):\n",
    "        \"\"\"\n",
    "        Load the replay buffer\n",
    "        \"\"\"\n",
    "        self.actions = np.load(folder_name + '/actions.npy')\n",
    "        self.frames = np.load(folder_name + '/frames.npy')\n",
    "        self.rewards = np.load(folder_name + '/rewards.npy')\n",
    "        self.terminal_flags = np.load(folder_name + '/terminal_flags.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
