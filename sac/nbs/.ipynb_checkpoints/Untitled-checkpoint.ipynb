{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Dense \n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_0 = hidden_0\n",
    "        self.hidden_1 = hidden_1\n",
    "        self.net_name = name\n",
    "\n",
    "        self.dense_0 = Dense(self.hidden_0, activation='relu')\n",
    "        self.dense_1 = Dense(self.hidden_1, activation='relu')\n",
    "        self.q_value = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        state_action_value = self.dense_0(tf.concat([state, action], axis=1))\n",
    "        state_action_value = self.dense_1(state_action_value)\n",
    "\n",
    "        q_value = self.q_value(state_action_value)\n",
    "\n",
    "        return q_value\n",
    "\n",
    "class CriticValue(tf.keras.Model):\n",
    "    def __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):\n",
    "        super(CriticValue, self).__init__()\n",
    "        self.hidden_0 = hidden_0\n",
    "        self.hidden_1 = hidden_1\n",
    "        self.net_name = name\n",
    "        \n",
    "        self.dense_0 = Dense(self.hidden_0, activation='relu')\n",
    "        self.dense_1 = Dense(self.hidden_1, activation='relu')\n",
    "        self.value = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        value = self.dense_0(state)\n",
    "        value = self.dense_1(value)\n",
    "\n",
    "        value = self.value(value)\n",
    "\n",
    "        return value\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, name, upper_bound, actions_dim, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1, noise=NOISE):\n",
    "        super(Actor, self).__init__()\n",
    "        self.hidden_0 = hidden_0\n",
    "        self.hidden_1 = hidden_1\n",
    "        self.actions_dim = actions_dim\n",
    "        self.net_name = name\n",
    "        self.upper_bound = upper_bound\n",
    "        self.noise = noise\n",
    "\n",
    "        self.dense_0 = Dense(self.hidden_0, activation='relu')\n",
    "        self.dense_1 = Dense(self.hidden_1, activation='relu')\n",
    "        self.mean = Dense(self.actions_dim, activation=None)\n",
    "        self.std = Dense(self.actions_dim, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        policy = self.dense_0(state)\n",
    "        policy = self.dense_1(policy)\n",
    "\n",
    "        mean = self.mean(policy)\n",
    "        std = self.std(policy)\n",
    "\n",
    "        std = tf.clip_by_value(std, self.noise, 1)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def get_action_log_probs(self, state, reparameterization_trick=True):\n",
    "        mean, std = self.call(state)\n",
    "        normal_distr = tfp.distributions.Normal(mean, std)\n",
    "\n",
    "        if reparameterization_trick:\n",
    "            actions = normal_distr.sample()\n",
    "        else:\n",
    "            actions = normal_distr.sample()\n",
    "\n",
    "        action = tf.math.tanh(actions) * self.upper_bound\n",
    "        log_probs = normal_distr.log_prob(actions) - tf.math.log(1-tf.math.pow(action,2)+self.noise)\n",
    "        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)\n",
    "\n",
    "        return action, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR, gamma=GAMMA, tau=TAU, reward_scale=REWARD_SCALE):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = ReplayBuffer(env)\n",
    "        self.actions_dim = env.action_space.shape[0]\n",
    "        self.upper_bound = env.action_space.high[0]\n",
    "        self.lower_bound = env.action_space.low[0]\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "\n",
    "        self.actor = Actor(actions_dim=self.actions_dim, name='actor', \n",
    "                                    upper_bound=env.action_space.high)\n",
    "        self.critic_0 = Critic(name='critic_0')\n",
    "        self.critic_1 = Critic(name='critic_1')\n",
    "        self.critic_value = CriticValue(name='value')\n",
    "        self.critic_target_value = CriticValue(name='target_value')\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=self.actor_lr))\n",
    "        self.critic_0.compile(optimizer=Adam(learning_rate=self.critic_lr))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate=self.critic_lr))\n",
    "        self.critic_value.compile(optimizer=Adam(learning_rate=self.critic_lr))\n",
    "        self.critic_target_value.compile(optimizer=Adam(learning_rate=self.critic_lr))\n",
    "\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "        self.critic_target_value.set_weights(self.critic_value.weights)\n",
    "        \n",
    "    def update_target_networks(self, tau):\n",
    "        critic_value_weights = self.critic_value.weights\n",
    "        critic_target_value_weights = self.critic_target_value.weights\n",
    "        for index in range(len(critic_value_weights)):\n",
    "            critic_target_value_weights[index] = tau * critic_value_weights[index] + (1 - tau) * critic_target_value_weights[index]\n",
    "\n",
    "        self.critic_target_value.set_weights(critic_target_value_weights)\n",
    "        \n",
    "    def add_to_replay_buffer(self, state, action, reward, new_state, done):\n",
    "        self.replay_buffer.add_record(state, action, reward, new_state, done)\n",
    "        \n",
    "    def save(self):\n",
    "        date_now = time.strftime(\"%Y%m%d%H%M\")\n",
    "        if not os.path.isdir(f\"{self.path_save}/save_agent_{date_now}\"):\n",
    "            os.makedirs(f\"{self.path_save}/save_agent_{date_now}\")\n",
    "        self.actor.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.actor.net_name}.h5\")\n",
    "        self.critic_0.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.critic_0.net_name}.h5\")\n",
    "        self.critic_1.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.critic_1.net_name}.h5\")\n",
    "        self.value_critic.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.value_critic.net_name}.h5\")\n",
    "        self.target_value_critic.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.target_value_critic.net_name}.h5\")\n",
    "        \n",
    "        self.replay_buffer.save(f\"{self.path_save}/save_agent_{date_now}\")\n",
    "\n",
    "    def load(self):\n",
    "        self.actor.load_weights(f\"{self.path_load}/{self.actor.net_name}.h5\")\n",
    "        self.critic_0.load_weights(f\"{self.path_load}/{self.critic_0.net_name}.h5\")\n",
    "        self.critic_1.load_weights(f\"{self.path_load}/{self.critic_1.net_name}.h5\")\n",
    "        self.value_critic.load_weights(f\"{self.path_load}/{self.value_critic.net_name}.h5\")\n",
    "        self.target_value_critic.load_weights(f\"{self.path_load}/{self.target_value_critic.net_name}.h5\")\n",
    "        \n",
    "        self.replay_buffer.load(f\"{self.path_load}\")\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions, _ = self.actor.get_action_log_probs(state, reparameterization_trick=False)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replay_buffer.check_buffer_size() == False:\n",
    "            return\n",
    "\n",
    "        state, action, reward, new_state, done = self.replay_buffer.get_minibatch()\n",
    "\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        new_states = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = tf.squeeze(self.critic_value(states), 1)\n",
    "            target_value = tf.squeeze(self.critic_target_value(new_states), 1)\n",
    "\n",
    "            policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=False)\n",
    "            log_probs = tf.squeeze(log_probs,1)\n",
    "            q_value_0 = self.critic_0(states, policy_actions)\n",
    "            q_value_1 = self.critic_1(states, policy_actions)\n",
    "            q_value = tf.squeeze(tf.math.minimum(q_value_0, q_value_1), 1)\n",
    "\n",
    "            value_target = q_value - log_probs\n",
    "            value_critic_loss = 0.5 * keras.losses.MSE(value, value_target)\n",
    "\n",
    "        value_critic_gradient = tape.gradient(value_critic_loss, self.critic_value.trainable_variables)\n",
    "        self.critic_value.optimizer.apply_gradients(zip(value_critic_gradient, self.critic_value.trainable_variables))\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=True)\n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            new_q_value_0 = self.critic_0(states, new_policy_actions)\n",
    "            new_q_value_1 = self.critic_1(states, new_policy_actions)\n",
    "            new_q_value = tf.squeeze(tf.math.minimum(new_q_value_0, new_q_value_1), 1)\n",
    "        \n",
    "            actor_loss = log_probs - new_q_value\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))\n",
    "        \n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            q_pred = self.reward_scale * reward + self.gamma * target_value * (1-done)\n",
    "            old_q_value_0 = tf.squeeze(self.critic_0(state, action), 1)\n",
    "            old_q_value_1 = tf.squeeze(self.critic_1(state, action), 1)\n",
    "            critic_0_loss = 0.5 * keras.losses.MSE(old_q_value_0, q_pred)\n",
    "            critic_1_loss = 0.5 * keras.losses.MSE(old_q_value_1, q_pred)\n",
    "    \n",
    "        critic_0_network_gradient = tape.gradient(critic_0_loss, self.critic_0.trainable_variables)\n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n",
    "\n",
    "        self.critic_0.optimizer.apply_gradients(zip(critic_0_network_gradient, self.critic_0.trainable_variables))\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "\n",
    "        self.update_target_networks(tau=self.tau)\n",
    "        \n",
    "        self.replay_buffer.update_n_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env = gym.make('InvertedPendulumBulletEnv-v0')\n",
    "agent = Agent(env=env)\n",
    "n_games = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 44.0 avg_score 44.0\n",
      "episode  1 score 43.0 avg_score 43.5\n",
      "episode  2 score 21.0 avg_score 36.0\n",
      "episode  3 score 41.0 avg_score 37.2\n",
      "episode  4 score 16.0 avg_score 33.0\n",
      "episode  5 score 20.0 avg_score 30.8\n",
      "episode  6 score 19.0 avg_score 29.1\n",
      "episode  7 score 16.0 avg_score 27.5\n",
      "episode  8 score 20.0 avg_score 26.7\n",
      "episode  9 score 19.0 avg_score 25.9\n",
      "episode  10 score 32.0 avg_score 26.5\n",
      "episode  11 score 31.0 avg_score 26.8\n",
      "episode  12 score 11.0 avg_score 25.6\n",
      "episode  13 score 31.0 avg_score 26.0\n",
      "episode  14 score 48.0 avg_score 27.5\n",
      "episode  15 score 15.0 avg_score 26.7\n",
      "episode  16 score 112.0 avg_score 31.7\n",
      "episode  17 score 206.0 avg_score 41.4\n",
      "episode  18 score 78.0 avg_score 43.3\n",
      "episode  19 score 142.0 avg_score 48.2\n",
      "episode  20 score 96.0 avg_score 50.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-80b073598af1>\", line 19, in <module>\n",
      "    agent.learn()\n",
      "  File \"<ipython-input-6-822884a28e2b>\", line 82, in learn\n",
      "    policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=False)\n",
      "  File \"<ipython-input-3-3bff3b1f0ef2>\", line 72, in get_action_log_probs\n",
      "    mean, std = self.call(state)\n",
      "  File \"<ipython-input-3-3bff3b1f0ef2>\", line 64, in call\n",
      "    mean = self.mean(policy)\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 996, in __call__\n",
      "    inputs = self._maybe_cast_inputs(inputs, input_list)\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2427, in _maybe_cast_inputs\n",
      "    any(map(self._should_cast_single_input, input_list))):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/anton/.conda/envs/reinforcement_learning/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-80b073598af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-822884a28e2b>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mpolicy_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreparameterization_trick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3bff3b1f0ef2>\u001b[0m in \u001b[0;36mget_action_log_probs\u001b[0;34m(self, state, reparameterization_trick)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreparameterization_trick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mnormal_distr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3bff3b1f0ef2>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    995\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autocast\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_cast_inputs\u001b[0;34m(self, inputs, input_list)\u001b[0m\n\u001b[1;32m   2426\u001b[0m     if (should_autocast and\n\u001b[0;32m-> 2427\u001b[0;31m         any(map(self._should_cast_single_input, input_list))):\n\u001b[0m\u001b[1;32m   2428\u001b[0m       \u001b[0;31m# Only perform expensive `nest` operation when needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement_learning/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "load_checkpoint = False\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    env.render(mode='human')\n",
    "\n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.add_to_replay_buffer(observation, action, reward, observation_, done)\n",
    "        if not load_checkpoint:\n",
    "            agent.learn()\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "    print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
