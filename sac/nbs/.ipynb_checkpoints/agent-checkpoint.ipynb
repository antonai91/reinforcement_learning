{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from plugin_write_and_run import *\n",
    "from tqdm import tqdm\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/agent.py\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers as opt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from config import *\n",
    "from replay_buffer import *\n",
    "from networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/agent.py\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, path_save=PATH_SAVE, path_load=PATH_LOAD, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR, gamma=GAMMA, tau=TAU, reward_scale=REWARD_SCALE):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = ReplayBuffer(env)\n",
    "        self.actions_dim = env.action_space.shape[0]\n",
    "        self.upper_bound = env.action_space.high[0]\n",
    "        self.lower_bound = env.action_space.low[0]\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.path_save = path_save\n",
    "        self.path_load = path_load\n",
    "\n",
    "        self.actor = Actor(actions_dim=self.actions_dim, name='actor', upper_bound=env.action_space.high)\n",
    "        self.critic_0 = Critic(name='critic_0')\n",
    "        self.critic_1 = Critic(name='critic_1')\n",
    "        self.critic_value = CriticValue(name='value')\n",
    "        self.critic_target_value = CriticValue(name='target_value')\n",
    "\n",
    "        self.actor.compile(optimizer=opt.Adam(learning_rate=self.actor_lr))\n",
    "        self.critic_0.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))\n",
    "        self.critic_1.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))\n",
    "        self.critic_value.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))\n",
    "        self.critic_target_value.compile(optimizer=opt.Adam(learning_rate=self.critic_lr))\n",
    "\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "        self.critic_target_value.set_weights(self.critic_value.weights)\n",
    "        \n",
    "    def update_target_networks(self, tau):\n",
    "        critic_value_weights = self.critic_value.weights\n",
    "        critic_target_value_weights = self.critic_target_value.weights\n",
    "        for index in range(len(critic_value_weights)):\n",
    "            critic_target_value_weights[index] = tau * critic_value_weights[index] + (1 - tau) * critic_target_value_weights[index]\n",
    "\n",
    "        self.critic_target_value.set_weights(critic_target_value_weights)\n",
    "        \n",
    "    def add_to_replay_buffer(self, state, action, reward, new_state, done):\n",
    "        self.replay_buffer.add_record(state, action, reward, new_state, done)\n",
    "        \n",
    "    def save(self):\n",
    "        date_now = time.strftime(\"%Y%m%d%H%M\")\n",
    "        if not os.path.isdir(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}\"):\n",
    "            os.makedirs(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}\")\n",
    "        self.actor.save_weights(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.actor.net_name}.h5\")\n",
    "        self.critic_0.save_weights(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_0.net_name}.h5\")\n",
    "        self.critic_1.save_weights(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_1.net_name}.h5\")\n",
    "        self.critic_value.save_weights(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_value.net_name}.h5\")\n",
    "        self.critic_target_value.save_weights(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}/{self.critic_target_value.net_name}.h5\")\n",
    "        \n",
    "        self.replay_buffer.save(f\"{self.path_save}/save_agent_{ENV_NAME.lower()}_{date_now}\")\n",
    "\n",
    "    def load(self):\n",
    "        self.actor.load_weights(f\"{self.path_load}/{self.actor.net_name}.h5\")\n",
    "        self.critic_0.load_weights(f\"{self.path_load}/{self.critic_0.net_name}.h5\")\n",
    "        self.critic_1.load_weights(f\"{self.path_load}/{self.critic_1.net_name}.h5\")\n",
    "        self.critic_value.load_weights(f\"{self.path_load}/{self.critic_value.net_name}.h5\")\n",
    "        self.critic_target_value.load_weights(f\"{self.path_load}/{self.critic_target_value.net_name}.h5\")\n",
    "        \n",
    "        #self.replay_buffer.load(f\"{self.path_load}\")\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions, _ = self.actor.get_action_log_probs(state, reparameterization_trick=False)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replay_buffer.check_buffer_size() == False:\n",
    "            return\n",
    "\n",
    "        state, action, reward, new_state, done = self.replay_buffer.get_minibatch()\n",
    "\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        new_states = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = tf.squeeze(self.critic_value(states), 1)\n",
    "            target_value = tf.squeeze(self.critic_target_value(new_states), 1)\n",
    "\n",
    "            policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=False)\n",
    "            log_probs = tf.squeeze(log_probs,1)\n",
    "            q_value_0 = self.critic_0(states, policy_actions)\n",
    "            q_value_1 = self.critic_1(states, policy_actions)\n",
    "            q_value = tf.squeeze(tf.math.minimum(q_value_0, q_value_1), 1)\n",
    "\n",
    "            value_target = q_value - log_probs\n",
    "            value_critic_loss = 0.5 * tf.keras.losses.MSE(value, value_target)\n",
    "\n",
    "        value_critic_gradient = tape.gradient(value_critic_loss, self.critic_value.trainable_variables)\n",
    "        self.critic_value.optimizer.apply_gradients(zip(value_critic_gradient, self.critic_value.trainable_variables))\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_policy_actions, log_probs = self.actor.get_action_log_probs(states, reparameterization_trick=True)\n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            new_q_value_0 = self.critic_0(states, new_policy_actions)\n",
    "            new_q_value_1 = self.critic_1(states, new_policy_actions)\n",
    "            new_q_value = tf.squeeze(tf.math.minimum(new_q_value_0, new_q_value_1), 1)\n",
    "        \n",
    "            actor_loss = log_probs - new_q_value\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))\n",
    "        \n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            q_pred = self.reward_scale * reward + self.gamma * target_value * (1-done)\n",
    "            old_q_value_0 = tf.squeeze(self.critic_0(state, action), 1)\n",
    "            old_q_value_1 = tf.squeeze(self.critic_1(state, action), 1)\n",
    "            critic_0_loss = 0.5 * tf.keras.losses.MSE(old_q_value_0, q_pred)\n",
    "            critic_1_loss = 0.5 * tf.keras.losses.MSE(old_q_value_1, q_pred)\n",
    "    \n",
    "        critic_0_network_gradient = tape.gradient(critic_0_loss, self.critic_0.trainable_variables)\n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n",
    "\n",
    "        self.critic_0.optimizer.apply_gradients(zip(critic_0_network_gradient, self.critic_0.trainable_variables))\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "\n",
    "        self.update_target_networks(tau=self.tau)\n",
    "        \n",
    "        self.replay_buffer.update_n_games()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
