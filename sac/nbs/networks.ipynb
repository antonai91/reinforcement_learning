{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from plugin_write_and_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/networks.py\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from replay_buffer import *\n",
    "from config import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/networks.py\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_0 = hidden_0\n",
    "        self.hidden_1 = hidden_1\n",
    "        self.net_name = name\n",
    "\n",
    "        self.dense_0 = Dense(self.hidden_0, activation='relu')\n",
    "        self.dense_1 = Dense(self.hidden_1, activation='relu')\n",
    "        self.q_value = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        state_action_value = self.dense_0(tf.concat([state, action], axis=1))\n",
    "        state_action_value = self.dense_1(state_action_value)\n",
    "\n",
    "        q_value = self.q_value(state_action_value)\n",
    "\n",
    "        return q_value\n",
    "\n",
    "class CriticValue(tf.keras.Model):\n",
    "    def __init__(self, name, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1):\n",
    "        super(CriticValue, self).__init__()\n",
    "        self.hidden_0 = hidden_0\n",
    "        self.hidden_1 = hidden_1\n",
    "        self.net_name = name\n",
    "        \n",
    "        self.dense_0 = Dense(self.hidden_0, activation='relu')\n",
    "        self.dense_1 = Dense(self.hidden_1, activation='relu')\n",
    "        self.value = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        value = self.dense_0(state)\n",
    "        value = self.dense_1(value)\n",
    "\n",
    "        value = self.value(value)\n",
    "\n",
    "        return value\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, name, upper_bound, actions_dim, hidden_0=CRITIC_HIDDEN_0, hidden_1=CRITIC_HIDDEN_1, noise=NOISE):\n",
    "        super(Actor, self).__init__()\n",
    "        self.hidden_0 = hidden_0\n",
    "        self.hidden_1 = hidden_1\n",
    "        self.actions_dim = actions_dim\n",
    "        self.net_name = name\n",
    "        self.upper_bound = upper_bound\n",
    "        self.noise = noise\n",
    "\n",
    "        self.dense_0 = Dense(self.hidden_0, activation='relu')\n",
    "        self.dense_1 = Dense(self.hidden_1, activation='relu')\n",
    "        self.mean = Dense(self.actions_dim, activation=None)\n",
    "        self.std = Dense(self.actions_dim, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        policy = self.dense_0(state)\n",
    "        policy = self.dense_1(policy)\n",
    "\n",
    "        mean = self.mean(policy)\n",
    "        std = self.std(policy)\n",
    "\n",
    "        std = tf.clip_by_value(std, self.noise, 1)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def get_action_log_probs(self, state, reparameterization_trick=True):\n",
    "        mean, std = self.call(state)\n",
    "        normal_distr = tfp.distributions.Normal(mean, std)\n",
    "\n",
    "        if reparameterization_trick:\n",
    "            actions = normal_distr.sample()\n",
    "        else:\n",
    "            actions = normal_distr.sample()\n",
    "\n",
    "        action = tf.math.tanh(actions) * self.upper_bound\n",
    "        log_probs = normal_distr.log_prob(actions) - tf.math.log(1-tf.math.pow(action,2)+self.noise)\n",
    "        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)\n",
    "\n",
    "        return action, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LunarLanderContinuous-v2\n"
     ]
    }
   ],
   "source": [
    "print(ENV_NAME)\n",
    "env = gym.make(ENV_NAME)\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "actor = Actor(\"name\", action_dim, upper_bound)\n",
    "critic = Critic(\"name\")\n",
    "critic_value = CriticValue(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ReplayBuffer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.array([-0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "state, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, log_probs = actor.get_action_log_probs(state[None, :], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic(state[None, :], action).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_value(state[None, :]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
