{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from config import *\n",
    "from pong_wrapper import *\n",
    "from process_image import *\n",
    "from utilities import *\n",
    "from plugin_write_and_run import *\n",
    "from a2c_networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/a2c_agent.py\n",
    "import tensorflow.keras.losses as kloss\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manton-ai\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/usr/local/lib/python3.6/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">classic-haze-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/anton-ai/tensorflow2_pong_a2c\" target=\"_blank\">https://wandb.ai/anton-ai/tensorflow2_pong_a2c</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/anton-ai/tensorflow2_pong_a2c/runs/1oqsp7py\" target=\"_blank\">https://wandb.ai/anton-ai/tensorflow2_pong_a2c/runs/1oqsp7py</a><br/>\n",
       "                Run data is saved locally in <code>/home/anton/Projects/reinforcement_learning/a2c/nbs/wandb/run-20210109_202753-1oqsp7py</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1oqsp7py)</h1><p></p><iframe src=\"https://wandb.ai/anton-ai/tensorflow2_pong_a2c/runs/1oqsp7py\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f018bcc5cc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict (\n",
    "  learning_rate = 0.00025,\n",
    "  batch_size = 64,\n",
    "  architecture = \"a2c\",\n",
    "  infra = \"Ubuntu\"\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "  project=\"tensorflow2_pong_a2c\",\n",
    "  tags=[\"a2c\", \"CNN\", \"RL\"],\n",
    "  config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = PongWrapper(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/a2c_agent.py\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, model, save_path=None, load_path=None, lr=1e-4, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
    "        # `gamma` is the discount factor\n",
    "        self.gamma = gamma\n",
    "        # Coefficients are used for the loss terms.\n",
    "        self.value_c = value_c\n",
    "        self.entropy_c = entropy_c\n",
    "        # `gamma` is the discount factor\n",
    "        self.gamma = gamma\n",
    "        self.save_path = save_path\n",
    "        self.load_path = load_path\n",
    "        \n",
    "        self.model = model\n",
    "        self.model.compile(\n",
    "                optimizer=opt.RMSprop(lr=lr),\n",
    "                # Define separate losses for policy logits and value estimate.\n",
    "                loss=[self._logits_loss, self._value_loss])\n",
    "        if load_path is not None:\n",
    "            print(\"loading model in {}\".format(load_path))\n",
    "            self.load_model(load_path)\n",
    "            print(\"model loaded\")\n",
    "            \n",
    "            \n",
    "    def train(self, wrapper, batch_sz=64, updates=1000000, input_shape=(84, 84, 4)):\n",
    "        # Storage helpers for a single batch of data.\n",
    "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, batch_sz))\n",
    "        observations = np.empty((batch_sz,) + input_shape)\n",
    "\n",
    "        # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "        ep_rewards = [0.0]\n",
    "        next_obs = wrapper.reset()\n",
    "        for update in range(updates):\n",
    "            start_time = time.time()\n",
    "            for step in range(batch_sz):\n",
    "                observations[step] = next_obs.copy()\n",
    "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "                next_processed_image, rewards[step], dones[step], _ = wrapper.step(actions[step], \"rgb_array\")\n",
    "                next_obs = wrapper.state\n",
    "                ep_rewards[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rewards.append(0.0)\n",
    "                    next_obs = wrapper.reset()\n",
    "                    wandb.log({'Game number': len(ep_rewards) - 1, '# Update': update, '% Update': round(update / updates, 2), \n",
    "                                \"Reward\": round(ep_rewards[-2], 2), \"Time taken\": round(time.time() - start_time, 2)})\n",
    "                        \n",
    "\n",
    "            _, next_value = self.model.action_value(next_obs[None, :])\n",
    "\n",
    "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "            # A trick to input actions and advantages through same API.\n",
    "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "\n",
    "            # Performs a full training step on the collected batch.\n",
    "            # Note: no need to mess around with gradients, Keras API handles it.\n",
    "            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "            \n",
    "            if update % 10000 == 0 and self.save_path is not None:\n",
    "                self.save_model(f'{self.save_path}/save_agent_{time.strftime(\"%Y%m%d%H%M\") + \"_\" + str(update).zfill(8)}')\n",
    "\n",
    "        return ep_rewards\n",
    "\n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        # `next_value` is the bootstrap value estimate of the future state (critic).\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "\n",
    "        # Returns are calculated as discounted sum of future rewards.\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
    "        returns = returns[:-1]\n",
    "\n",
    "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "        advantages = returns - values\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "    def test(self, wrapper, render=True):\n",
    "        obs, done, ep_reward = wrapper.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(obs[None, :])\n",
    "            processed_image, reward, done, _ = wrapper.step(action, \"rgb_array\")\n",
    "            obs = wrapper.state\n",
    "            ep_reward += reward\n",
    "        return ep_reward\n",
    "\n",
    "    def _value_loss(self, returns, value):\n",
    "        # Value loss is typically MSE between value estimates and returns.\n",
    "        return self.value_c * kloss.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, actions_and_advantages, logits):\n",
    "        # A trick to input actions and advantages through the same API.\n",
    "        actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
    "\n",
    "        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "        # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "        weighted_sparse_ce = kloss.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "        # Note: we only calculate the loss on the actions we've actually taken.\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "\n",
    "        # Entropy loss can be calculated as cross-entropy over itself.\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        entropy_loss = kloss.categorical_crossentropy(probs, probs)\n",
    "\n",
    "        # We want to minimize policy and maximize entropy losses.\n",
    "        # Here signs are flipped because the optimizer minimizes.\n",
    "        return policy_loss - self.entropy_c * entropy_loss\n",
    "    \n",
    "    # To-do\n",
    "    def save_model(self, folder_path):\n",
    "        if not os.path.isdir(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        self.model.save_weights(folder_path + '/a2c')\n",
    "                              \n",
    "    \n",
    "    def load_model(self, folder_path):\n",
    "        self.model.load_weights(folder_path + '/a2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_actions=pw.env.action_space.n, hidden=HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model in ../model/save_agent_202101092027\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(model, load_path=\"../model/save_agent_202101092027\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model in ../model/save_agent_202101092039\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    " try:\n",
    "     rewards_history = agent.train(pw)\n",
    "\n",
    " except KeyboardInterrupt:\n",
    "     # Save the model, I need this in order to save the networks, frame number, rewards and losses. \n",
    "     # if I want to stop the script and restart without training from the beginning\n",
    "     if PATH_SAVE_MODEL is None:\n",
    "         print(\"Setting path to ../model\")\n",
    "         PATH_SAVE_MODEL = \"../model\"\n",
    "     print('Saving the model in ' + f'{PATH_SAVE_MODEL}/save_agent_{time.strftime(\"%Y%m%d%H%M\")}')\n",
    "     agent.save_model(f'{PATH_SAVE_MODEL}/save_agent_{time.strftime(\"%Y%m%d%H%M\")}')\n",
    "     print('Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
