{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from plugin_write_and_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/ppo_agent.py\n",
    "import numpy as np\n",
    "import time\n",
    "import wandb\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import mean_squared_error, categorical_crossentropy\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/ppo_agent.py\n",
    "\n",
    "class PpoAgent():\n",
    "    def __init__(self, model, save_path=PATH_SAVE_MODEL, load_path=PATH_LOAD_MODEL, gamma=GAMMA, max_updates=MAX_UPDATES, batch_size=BATCH_SIZE, input_shape=INPUT_SHAPE+(HISTORY_LENGHT, ), \n",
    "                 clip_ratio=CLIP_RATIO, entropy_c=ENTROPY_C, value_c=VALUE_C, lr=LR):\n",
    "        # `gamma` is the discount factor\n",
    "        self.gamma = gamma\n",
    "        self.max_updates = max_updates\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.save_path = save_path\n",
    "        self.load_path = load_path\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.entropy_c = entropy_c\n",
    "        self.value_c = value_c\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.model = model\n",
    "        self.opt = Adam(self.lr)\n",
    "        \n",
    "        if load_path is not None:\n",
    "            print(\"loading model in {}\".format(load_path))\n",
    "            self.load_model(load_path)\n",
    "            print(\"model loaded\")\n",
    "        \n",
    "        \"\"\"\n",
    "        if load_path is not None:\n",
    "            print(\"loading model in {}\".format(load_path))\n",
    "            self.load_model(load_path)\n",
    "            print(\"model loaded\")\n",
    "        \"\"\"\n",
    "    \n",
    "    def logit_loss(self, old_policy, new_policy, actions_oh, advs):\n",
    "        advs = tf.stop_gradient(advs)\n",
    "        advs =  tf.cast(advs, tf.float32)\n",
    "        old_log_p = tf.math.log(\n",
    "            tf.reduce_sum(old_policy * actions_oh))\n",
    "        old_log_p = tf.stop_gradient(old_log_p)\n",
    "        log_p = tf.math.log(tf.reduce_sum(\n",
    "            new_policy * actions_oh))\n",
    "        ratio = tf.math.exp(log_p - old_log_p)\n",
    "        clipped_ratio = tf.clip_by_value(\n",
    "            ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "        surrogate = -tf.minimum(ratio * advs, self.clip_ratio * advs)\n",
    "        return tf.reduce_mean(surrogate) - self.entropy_c * categorical_crossentropy(new_policy, new_policy)\n",
    "    \n",
    "    def value_loss(self, returns, values):\n",
    "        return self.value_c * mean_squared_error(returns, values)\n",
    "    \n",
    "    def get_prob_value_action(self, state):\n",
    "        prob, value = self.model(state[None, :])\n",
    "        return prob, value, tf.squeeze(tf.random.categorical(prob, 1), axis=-1)\n",
    "    \n",
    "    def get_returns_advantages(self, rewards, dones, values, next_value):\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # Returns are calculated as discounted sum of future rewards.\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
    "        returns = returns[:-1]\n",
    "        \n",
    "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "        advantages = returns - values\n",
    "        \n",
    "        return returns, advantages\n",
    "        \n",
    "    def train(self, game_wrapper):\n",
    "        \n",
    "        probs = np.empty((self.batch_size, game_wrapper.env.action_space.n))\n",
    "        actions = np.empty((self.batch_size,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, self.batch_size))\n",
    "        observations = np.empty((self.batch_size,) + self.input_shape)\n",
    "        \n",
    "        ep_rewards = [0.0]\n",
    "        next_obs = game_wrapper.reset()\n",
    "        \n",
    "        for update in tqdm(range(self.max_updates)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for step in range(self.batch_size):\n",
    "                observations[step] = next_obs.copy()\n",
    "                prob, value, action = self.get_prob_value_action(next_obs)\n",
    "                actions[step] = action\n",
    "                probs[step] = prob\n",
    "                values[step] = value\n",
    "                next_processed_image, rewards[step], dones[step], _ = game_wrapper.step(actions[step], \"rgb_array\")\n",
    "                next_obs = game_wrapper.state\n",
    "                ep_rewards[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rewards.append(0.0)\n",
    "                    next_obs = game_wrapper.reset()\n",
    "                    wandb.log({'Game number': len(ep_rewards) - 1, '# Update': update, '% Update': round(update / self.max_updates, 2), \n",
    "                                \"Reward\": round(ep_rewards[-2], 2), \"Time taken\": round(time.time() - start_time, 2)})\n",
    "            \n",
    "            _, next_value = self.model(next_obs[None, :])\n",
    "            next_value = np.squeeze(next_value, -1)\n",
    "            \n",
    "            returns, advs = self.get_returns_advantages(rewards, dones, values, next_value)\n",
    "            \n",
    "            actions_oh = tf.one_hot(actions, game_wrapper.env.action_space.n)\n",
    "            actions_oh = tf.reshape(actions_oh, [-1, game_wrapper.env.action_space.n])\n",
    "            actions_oh = tf.cast(actions_oh, tf.float32)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                logits, v = self.model(observations, training=True)\n",
    "                logit_loss = self.logit_loss(probs, logits, actions_oh, advs)\n",
    "                value_loss = self.value_loss(returns, v)\n",
    "                value_loss = tf.cast(value_loss, tf.float32)\n",
    "                loss = logit_loss + value_loss\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            \n",
    "            if (update + 1) % 5000 == 0 and self.save_path is not None:\n",
    "                print(\"Saving model in {}\".format(self.save_path))\n",
    "                self.save_model(f'{self.save_path}/save_agent_{time.strftime(\"%Y%m%d%H%M\") + \"_\" + str(update).zfill(8)}')\n",
    "                print(\"model saved\")\n",
    "        \n",
    "        return ep_rewards\n",
    "\n",
    "    def save_model(self, folder_path):\n",
    "        if not os.path.isdir(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        self.model.save_weights(folder_path + '/ppo')\n",
    "                              \n",
    "    \n",
    "    def load_model(self, folder_path):\n",
    "        self.model.load_weights(folder_path + '/ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pong_wrapper import *\n",
    "from ppo_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = PongWrapper(ENV_NAME)\n",
    "model = PpoNetwork()\n",
    "ppo_agent = PpoAgent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manton-ai\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">resilient-microwave-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/anton-ai/tensorflow2_pong_ppo\" target=\"_blank\">https://wandb.ai/anton-ai/tensorflow2_pong_ppo</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/anton-ai/tensorflow2_pong_ppo/runs/1mlqrfka\" target=\"_blank\">https://wandb.ai/anton-ai/tensorflow2_pong_ppo/runs/1mlqrfka</a><br/>\n",
       "                Run data is saved locally in <code>/Users/anton/Projects/reinforcement_learning/ppo/nbs/wandb/run-20210129_122008-1mlqrfka</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1mlqrfka)</h1><p></p><iframe src=\"https://wandb.ai/anton-ai/tensorflow2_pong_ppo/runs/1mlqrfka\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe5cef93bb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "  project=\"tensorflow2_pong_ppo\",\n",
    "  tags=[\"a2c\", \"CNN\", \"RL\"],\n",
    "  config=CONFIG_WANDB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "rewards = ppo_agent.train(pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-21.0, -20.0, -20.0, -21.0, -20.0, -20.0, -15.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
