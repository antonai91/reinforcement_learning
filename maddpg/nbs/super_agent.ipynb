{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "civilian-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "olympic-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plugin_write_and_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "further-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/super_agent.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers as opt\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from config import *\n",
    "from make_env import *\n",
    "from replay_buffer import *\n",
    "from agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boring-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/super_agent.py\n",
    "\n",
    "class SuperAgent:\n",
    "    def __init__(self, env, path_save=PATH_SAVE_MODEL, path_load=PATH_LOAD_FOLDER):\n",
    "        self.path_save = path_save\n",
    "        self.path_load = path_load\n",
    "        self.replay_buffer = ReplayBuffer(env)\n",
    "        self.n_agents = len(env.agents)\n",
    "        self.agents = [Agent(env, agent) for agent in range(self.n_agents)]\n",
    "        \n",
    "    def get_actions(self, agents_states):\n",
    "        list_actions = [self.agents[index].get_actions(agents_states[index]) for index in range(self.n_agents)]\n",
    "        return list_actions\n",
    "    \n",
    "    def save(self):\n",
    "        date_now = time.strftime(\"%Y%m%d%H%M\")\n",
    "        full_path = f\"{self.path_save}/save_agent_{date_now}\"\n",
    "        if not os.path.isdir(full_path):\n",
    "            os.makedirs(full_path)\n",
    "        \n",
    "        for agent in self.agents:\n",
    "            agent.save(full_path)\n",
    "            \n",
    "        self.replay_buffer.save(full_path)\n",
    "    \n",
    "    def load(self):\n",
    "        full_path = self.path_load\n",
    "        for agent in self.agents:\n",
    "            agent.load(full_path)\n",
    "            \n",
    "        self.replay_buffer.load(full_path)\n",
    "    \n",
    "    def train(self):\n",
    "        if self.replay_buffer.check_buffer_size() == False:\n",
    "            return\n",
    "        \n",
    "        state, reward, next_state, done, actors_state, actors_next_state, actors_action = self.replay_buffer.get_minibatch()\n",
    "        \n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_state, dtype=tf.float32)\n",
    "        \n",
    "        actors_states = [tf.convert_to_tensor(s, dtype=tf.float32) for s in actors_state]\n",
    "        actors_next_states = [tf.convert_to_tensor(s, dtype=tf.float32) for s in actors_next_state]\n",
    "        actors_actions = [tf.convert_to_tensor(s, dtype=tf.float32) for s in actors_action]\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            target_actions = [self.agents[index].target_actor(actors_next_states[index]) for index in range(self.n_agents)]\n",
    "            policy_actions = [self.agents[index].actor(actors_states[index]) for index in range(self.n_agents)]\n",
    "            \n",
    "            concat_target_actions = tf.concat(target_actions, axis=1)\n",
    "            concat_policy_actions = tf.concat(policy_actions, axis=1)\n",
    "            concat_actors_action = tf.concat(actors_actions, axis=1)\n",
    "            \n",
    "            target_critic_values = [tf.squeeze(self.agents[index].target_critic(next_states, concat_target_actions), 1) for index in range(self.n_agents)]\n",
    "            critic_values = [tf.squeeze(self.agents[index].critic(states, concat_actors_action), 1) for index in range(self.n_agents)]\n",
    "            targets = [rewards[:, index] + self.agents[index].gamma * target_critic_values[index] * (1-done[:, index]) for index in range(self.n_agents)]\n",
    "            critic_losses = [tf.keras.losses.MSE(targets[index], critic_values[index]) for index in range(self.n_agents)]\n",
    "            \n",
    "            actor_losses = [-self.agents[index].critic(states, concat_policy_actions) for index in range(self.n_agents)]\n",
    "            actor_losses = [tf.math.reduce_mean(actor_losses[index]) for index in range(self.n_agents)]\n",
    "        \n",
    "        critic_gradients = [tape.gradient(critic_losses[index], self.agents[index].critic.trainable_variables) for index in range(self.n_agents)]\n",
    "        actor_gradients = [tape.gradient(actor_losses[index], self.agents[index].actor.trainable_variables) for index in range(self.n_agents)]\n",
    "        \n",
    "        for index in range(self.n_agents):\n",
    "            self.agents[index].critic.optimizer.apply_gradients(zip(critic_gradients[index], self.agents[index].critic.trainable_variables))\n",
    "            self.agents[index].actor.optimizer.apply_gradients(zip(actor_gradients[index], self.agents[index].actor.trainable_variables))\n",
    "            self.agents[index].update_target_networks(self.agents[index].tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dutch-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "knowing-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_states, reward, done, info = env.step([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focused-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SuperAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facial-mailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<agent.Agent at 0x7fcc9189ffd0>,\n",
       " <agent.Agent at 0x7fcc91aaef40>,\n",
       " <agent.Agent at 0x7fcc3bf7c0d0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mysterious-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01770053,  0.58493223,  0.04998703,  0.62523341, -0.22160163,\n",
       "        -0.54285523, -0.3109281 ,  0.02469857]),\n",
       " array([ 0.23930217,  1.12778746,  0.23930217,  1.12778746,  0.27158866,\n",
       "         1.16808863,  0.22160163,  0.54285523, -0.08932647,  0.5675538 ]),\n",
       " array([ 0.32862863,  0.56023366,  0.32862863,  0.56023366,  0.36091513,\n",
       "         0.60053483,  0.3109281 , -0.02469857,  0.08932647, -0.5675538 ])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "round-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    actors_state, reward, done, info = env.step([[0.1, 0.1, 0.2, 0.4, 0.5], [0.1, 0.1, 0.2, 0.4, 0.5], [0.1, 0.1, 0.2, 0.4, 0.5]])\n",
    "    state = np.concatenate(actors_state)\n",
    "    sa.replay_buffer.add_record(actors_state, actors_state, [[0.1, 0.1, 0.2, 0.4, 0.5], [0.1, 0.1, 0.2, 0.4, 0.5], [0.1, 0.1, 0.2, 0.4, 0.5]], state, state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "generic-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, next_state, done, actors_state, actors_next_state, actors_action = sa.replay_buffer.get_minibatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "developmental-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dried-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-transcript",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
